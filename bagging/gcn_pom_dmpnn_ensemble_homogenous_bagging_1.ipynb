{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:588: calling function (from tensorflow.python.eager.polymorphic_function.polymorphic_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "experimental_relax_shapes is deprecated, use reduce_retracing instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading modules with transformers dependency. No module named 'transformers'\n",
      "cannot import name 'HuggingFaceModel' from 'deepchem.models.torch_models' (c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\models\\torch_models\\__init__.py)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import deepchem as dc\n",
    "from deepchem.feat import GraphData\n",
    "import pandas as pd\n",
    "from deepchem.feat import DMPNNFeaturizer\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graphs(weighted_adj_matrix):\n",
    "    node_features = weighted_adj_matrix.diagonal().reshape(-1, 1)\n",
    "    edges = np.array(np.nonzero(weighted_adj_matrix))\n",
    "    edge_weights = weighted_adj_matrix[edges.T[:, 0], edges.T[:, 1]]\n",
    "    graph = GraphData(node_features=node_features, edge_index=edges, edge_features=edge_weights.reshape(-1, 1))\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of tasks:  138\n"
     ]
    }
   ],
   "source": [
    "TASKS = [\n",
    "'alcoholic', 'aldehydic', 'alliaceous', 'almond', 'amber', 'animal',\n",
    "'anisic', 'apple', 'apricot', 'aromatic', 'balsamic', 'banana', 'beefy',\n",
    "'bergamot', 'berry', 'bitter', 'black currant', 'brandy', 'burnt',\n",
    "'buttery', 'cabbage', 'camphoreous', 'caramellic', 'cedar', 'celery',\n",
    "'chamomile', 'cheesy', 'cherry', 'chocolate', 'cinnamon', 'citrus', 'clean',\n",
    "'clove', 'cocoa', 'coconut', 'coffee', 'cognac', 'cooked', 'cooling',\n",
    "'cortex', 'coumarinic', 'creamy', 'cucumber', 'dairy', 'dry', 'earthy',\n",
    "'ethereal', 'fatty', 'fermented', 'fishy', 'floral', 'fresh', 'fruit skin',\n",
    "'fruity', 'garlic', 'gassy', 'geranium', 'grape', 'grapefruit', 'grassy',\n",
    "'green', 'hawthorn', 'hay', 'hazelnut', 'herbal', 'honey', 'hyacinth',\n",
    "'jasmin', 'juicy', 'ketonic', 'lactonic', 'lavender', 'leafy', 'leathery',\n",
    "'lemon', 'lily', 'malty', 'meaty', 'medicinal', 'melon', 'metallic',\n",
    "'milky', 'mint', 'muguet', 'mushroom', 'musk', 'musty', 'natural', 'nutty',\n",
    "'odorless', 'oily', 'onion', 'orange', 'orangeflower', 'orris', 'ozone',\n",
    "'peach', 'pear', 'phenolic', 'pine', 'pineapple', 'plum', 'popcorn',\n",
    "'potato', 'powdery', 'pungent', 'radish', 'raspberry', 'ripe', 'roasted',\n",
    "'rose', 'rummy', 'sandalwood', 'savory', 'sharp', 'smoky', 'soapy',\n",
    "'solvent', 'sour', 'spicy', 'strawberry', 'sulfurous', 'sweaty', 'sweet',\n",
    "'tea', 'terpenic', 'tobacco', 'tomato', 'tropical', 'vanilla', 'vegetable',\n",
    "'vetiver', 'violet', 'warm', 'waxy', 'weedy', 'winey', 'woody'\n",
    "]\n",
    "print(\"No of tasks: \", len(TASKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from openpom.feat.graph_featurizer import GraphFeaturizer, GraphConvConstants\n",
    "from openpom.utils.data_utils import get_class_imbalance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.feat import DMPNNFeaturizer\n",
    "import deepchem as dc\n",
    "\n",
    "\n",
    "featurizer = DMPNNFeaturizer()\n",
    "\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "\n",
    "\n",
    "loader = dc.data.CSVLoader(\n",
    "    tasks=TASKS,  \n",
    "    feature_field=smiles_field,\n",
    "    featurizer=featurizer\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = loader.create_dataset(inputs=[\"without_O_DMPNNpruned_without hydrogen_curated_GS_LF_merged_4812_QM_cleaned.csv\"])\n",
    "n_tasks = len(dataset.tasks)\n",
    "dataset.X\n",
    "len(dataset.X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"without_O_DMPNNpruned_graph_data_cleaned.npz\",allow_pickle=True)\n",
    "mtx_list = data['mtx']\n",
    "qm_graphs = []\n",
    "for mtx in mtx_list:\n",
    "    qm_graphs.append(get_graphs(mtx))\n",
    "QM_X = np.asarray(qm_graphs)\n",
    "QM_X\n",
    "len(QM_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([i for i in zip(dataset.X, QM_X)])\n",
    "new_dataset = dc.data.NumpyDataset(X=X, y=dataset.y, n_tasks=138, ids=dataset.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (4811, 2), y.shape: (4811, 138), w.shape: (4811, 1), task_names: [  0   1   2 ... 135 136 137]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (4811, 2), y.shape: (4811, 138), w.shape: (4811, 1), task_names: [  0   1   2 ... 135 136 137]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "randomstratifiedsplitter = dc.splits.RandomStratifiedSplitter()\n",
    "train_dataset, test_dataset, valid_dataset = randomstratifiedsplitter.train_valid_test_split(new_dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3822\n",
      "492\n",
      "497\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (3822, 2), y.shape: (3822, 138), w.shape: (3822, 1), task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=train_dataset.X,\n",
    "                    y= train_dataset.y,\n",
    "                    w= train_dataset.w,\n",
    "                    ids=train_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./train_split_dmpnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (492, 2), y.shape: (492, 138), w.shape: (492, 1), ids: ['CCC(=O)C(=O)O' 'O=C(O)CCCCC(=O)O' 'O=C(O)c1ccccc1' ...\n",
       " 'CCCCCCCCCCC=CCOC(C)=O' 'CCCCCC=CCCC(OCC)OCC' 'CCC1SC(CC(C)C)=NC1C'], task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=valid_dataset.X,\n",
    "                    y= valid_dataset.y,\n",
    "                    w= valid_dataset.w,\n",
    "                    ids=valid_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./valid_split_dmpnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (497, 2), y.shape: (497, 138), w.shape: (497, 1), ids: ['CC(O)CN' 'O=Cc1ccccc1' 'CC(=O)C(C)=O' ... 'C=C(C)C1CC=C(C2OCC(C)O2)CC1'\n",
       " 'CCCC=CC1OCC(O)CO1' 'CC(C)C1CCC2CC(=O)CC(C)C2(C)C1'], task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=test_dataset.X,\n",
    "                    y= test_dataset.y,\n",
    "                    w= test_dataset.w,\n",
    "                    ids=test_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./test_split_dmpnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "\n",
    "from deepchem.models.losses import Loss, L2Loss\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from deepchem.models.optimizers import Optimizer, LearningRateSchedule\n",
    "\n",
    "from openpom.layers.pom_ffn import CustomPositionwiseFeedForward\n",
    "from openpom.utils.loss import CustomMultiLabelLoss\n",
    "from openpom.utils.optimizer import get_optimizer\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    from dgl import DGLGraph\n",
    "    from dgl.nn.pytorch import Set2Set\n",
    "    from openpom.layers.pom_mpnn_gnn import CustomMPNNGNN\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    raise ImportError('This module requires dgl and dgllife')\n",
    "\n",
    "\n",
    "class MPNNPOM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'classification',\n",
    "                 number_atom_features: int = 133,\n",
    "                 number_bond_features: int = 14,\n",
    "                 n_classes: int = 1,\n",
    "                 nfeat_name: str = 'x',\n",
    "                 efeat_name: str = 'edge_attr',\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True):\n",
    "\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        super(MPNNPOM, self).__init__()\n",
    "\n",
    "        self.n_tasks: int = n_tasks\n",
    "        self.mode: str = mode\n",
    "        self.n_classes: int = n_classes\n",
    "        self.nfeat_name: str = nfeat_name\n",
    "        self.efeat_name: str = efeat_name\n",
    "        self.readout_type: str = readout_type\n",
    "        self.ffn_embeddings: int = ffn_embeddings\n",
    "        self.ffn_activation: str = ffn_activation\n",
    "        self.ffn_dropout_p: float = ffn_dropout_p\n",
    "\n",
    "        if mode == 'classification':\n",
    "            self.ffn_output: int = n_tasks * n_classes\n",
    "        else:\n",
    "            self.ffn_output = n_tasks\n",
    "\n",
    "        self.mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=number_atom_features,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_in_feats=number_bond_features,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type)\n",
    "        \n",
    "        QM_node_out_features = 50\n",
    "        \n",
    "        self.QM_mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=1,\n",
    "            node_out_feats=QM_node_out_features,\n",
    "            edge_in_feats=1,\n",
    "            edge_hidden_feats=16,\n",
    "            num_step_message_passing=2,\n",
    "            residual=True,\n",
    "            message_aggregator_type='sum')\n",
    "\n",
    "        self.project_edge_feats: nn.Module = nn.Sequential(\n",
    "            nn.Linear(number_bond_features, edge_out_feats), nn.ReLU())\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            self.readout_set2set: nn.Module = Set2Set(\n",
    "                input_dim=node_out_feats + edge_out_feats,\n",
    "                n_iters=num_step_set2set,\n",
    "                n_layers=num_layer_set2set)\n",
    "            ffn_input: int = 2 * (node_out_feats + edge_out_feats)\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            ffn_input = node_out_feats + edge_out_feats\n",
    "        else:\n",
    "            raise Exception(\"readout_type invalid\")\n",
    "\n",
    "        if ffn_embeddings is not None:\n",
    "            d_hidden_list: List = ffn_hidden_list + [ffn_embeddings]\n",
    "\n",
    "        self.ffn: nn.Module = CustomPositionwiseFeedForward(\n",
    "            d_input=ffn_input + QM_node_out_features,\n",
    "            \n",
    "            d_hidden_list=d_hidden_list,\n",
    "            d_output=self.ffn_output,\n",
    "            activation=ffn_activation,\n",
    "            dropout_p=ffn_dropout_p,\n",
    "            dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "    def _readout(self, g: DGLGraph, node_encodings: torch.Tensor,\n",
    "                 edge_feats: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        g.ndata['node_emb'] = node_encodings\n",
    "        g.edata['edge_emb'] = self.project_edge_feats(edge_feats)\n",
    "\n",
    "        def message_func(edges) -> Dict:\n",
    "            \"\"\"\n",
    "            The message function to generate messages\n",
    "            along the edges for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg: torch.Tensor = torch.cat(\n",
    "                (edges.src['node_emb'], edges.data['edge_emb']), dim=1)\n",
    "            return {'src_msg': src_msg}\n",
    "\n",
    "        def reduce_func(nodes) -> Dict:\n",
    "            \"\"\"\n",
    "            The reduce function to aggregate the messages\n",
    "            for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg_sum: torch.Tensor = torch.sum(nodes.mailbox['src_msg'],\n",
    "                                                  dim=1)\n",
    "            return {'src_msg_sum': src_msg_sum}\n",
    "\n",
    "        \n",
    "        g.send_and_recv(g.edges(),\n",
    "                        message_func=message_func,\n",
    "                        reduce_func=reduce_func)\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            batch_mol_hidden_states: torch.Tensor = self.readout_set2set(\n",
    "                g, g.ndata['src_msg_sum'])\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            batch_mol_hidden_states = dgl.sum_nodes(g, 'src_msg_sum')\n",
    "\n",
    "        \n",
    "        return batch_mol_hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self, graphs: tuple[DGLGraph]\n",
    "    ) -> Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        g, qm_g = graphs\n",
    "        node_feats: torch.Tensor = g.ndata[self.nfeat_name]\n",
    "        edge_feats: torch.Tensor = g.edata[self.efeat_name]\n",
    "\n",
    "        qm_node_feats: torch.Tensor = qm_g.ndata[self.nfeat_name]\n",
    "        qm_edge_feats: torch.Tensor = qm_g.edata[self.efeat_name]\n",
    "\n",
    "        node_encodings: torch.Tensor = self.mpnn(g, node_feats, edge_feats)\n",
    "\n",
    "        QM_encodings: torch.Tensor = self.QM_mpnn(qm_g, qm_node_feats, qm_edge_feats)\n",
    "        qm_g.ndata['node_emb'] = QM_encodings\n",
    "        molecular_QM_encodings = dgl.sum_nodes(qm_g, 'node_emb')\n",
    "        molecular_encodings: torch.Tensor = self._readout(\n",
    "            g, node_encodings, edge_feats)\n",
    "        if self.readout_type == 'global_sum_pooling':\n",
    "            molecular_encodings = F.softmax(molecular_encodings, dim=1)\n",
    "\n",
    "        embeddings: torch.Tensor\n",
    "        out: torch.Tensor\n",
    "        embeddings, out = self.ffn(torch.concat((molecular_encodings, molecular_QM_encodings), dim=1))\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            if self.n_tasks == 1:\n",
    "                logits: torch.Tensor = out.view(-1, self.n_classes)\n",
    "            else:\n",
    "                logits = out.view(-1, self.n_tasks, self.n_classes)\n",
    "            proba: torch.Tensor = F.sigmoid(\n",
    "                logits)  \n",
    "            if self.n_classes == 1:\n",
    "                proba = proba.squeeze(-1)  \n",
    "            return proba, logits, embeddings\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class MPNNPOMModel(TorchModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 class_imbalance_ratio: Optional[List] = None,\n",
    "                 loss_aggr_type: str = 'sum',\n",
    "                 learning_rate: Union[float, LearningRateSchedule] = 0.001,\n",
    "                 batch_size: int = 25,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'regression',\n",
    "                 number_atom_features: int = 133,\n",
    "                 number_bond_features: int = 14,\n",
    "                 n_classes: int = 1,\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True,\n",
    "                 weight_decay: float = 1e-5,\n",
    "                 self_loop: bool = False,\n",
    "                 optimizer_name: str = 'adam',\n",
    "                 device_name: Optional[str] = None,\n",
    "                 **kwargs):\n",
    "        model: nn.Module = MPNNPOM(\n",
    "            n_tasks=n_tasks,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            edge_out_feats=edge_out_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            mpnn_residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type,\n",
    "            mode=mode,\n",
    "            number_atom_features=number_atom_features,\n",
    "            number_bond_features=number_bond_features,\n",
    "            n_classes=n_classes,\n",
    "            readout_type=readout_type,\n",
    "            num_step_set2set=num_step_set2set,\n",
    "            num_layer_set2set=num_layer_set2set,\n",
    "            ffn_hidden_list=ffn_hidden_list,\n",
    "            ffn_embeddings=ffn_embeddings,\n",
    "            ffn_activation=ffn_activation,\n",
    "            ffn_dropout_p=ffn_dropout_p,\n",
    "            ffn_dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "        if class_imbalance_ratio and (len(class_imbalance_ratio) != n_tasks):\n",
    "            raise Exception(\"size of class_imbalance_ratio \\\n",
    "                            should be equal to n_tasks\")\n",
    "\n",
    "        if mode == 'regression':\n",
    "            loss: Loss = L2Loss()\n",
    "            output_types: List = ['prediction']\n",
    "        else:\n",
    "            loss = CustomMultiLabelLoss(\n",
    "                class_imbalance_ratio=class_imbalance_ratio,\n",
    "                loss_aggr_type=loss_aggr_type,\n",
    "                device=device_name)\n",
    "            output_types = ['prediction', 'loss', 'embedding']\n",
    "\n",
    "        optimizer: Optimizer = get_optimizer(optimizer_name)\n",
    "        optimizer.learning_rate = learning_rate\n",
    "        if device_name is not None:\n",
    "            device: Optional[torch.device] = torch.device(device_name)\n",
    "        else:\n",
    "            device = None\n",
    "        super(MPNNPOMModel, self).__init__(model,\n",
    "                                           loss=loss,\n",
    "                                           output_types=output_types,\n",
    "                                           optimizer=optimizer,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           device=device,\n",
    "                                           **kwargs)\n",
    "\n",
    "        self.weight_decay: float = weight_decay\n",
    "        self._self_loop: bool = self_loop\n",
    "        self.regularization_loss: Callable = self._regularization_loss\n",
    "\n",
    "    def _regularization_loss(self) -> torch.Tensor:\n",
    "        l1_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        l2_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                l1_regularization = l1_regularization + torch.norm(param, p=1)\n",
    "                l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
    "        l1_norm: torch.Tensor = self.weight_decay * l1_regularization\n",
    "        l2_norm: torch.Tensor = self.weight_decay * l2_regularization\n",
    "        return l1_norm + l2_norm\n",
    "\n",
    "    def _prepare_batch(\n",
    "        self, batch: Tuple[List, List, List]\n",
    "    ) -> Tuple[DGLGraph, List[torch.Tensor], List[torch.Tensor]]:\n",
    "        inputs: List\n",
    "        labels: List\n",
    "        weights: List\n",
    "\n",
    "        inputs, labels, weights = batch\n",
    "        dgl_graphs1: List[DGLGraph] = [\n",
    "            graphs[0].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graphs in inputs[0]\n",
    "        ]\n",
    "        g1: DGLGraph = dgl.batch(dgl_graphs1).to(self.device)\n",
    "\n",
    "        dgl_graphs2: List[DGLGraph] = [\n",
    "            graphs[1].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graphs in inputs[0]\n",
    "        ]\n",
    "        g2: DGLGraph = dgl.batch(dgl_graphs2).to(self.device)\n",
    "        _, labels, weights = super(MPNNPOMModel, self)._prepare_batch(\n",
    "            ([], labels, weights))\n",
    "        return (g1, g2), labels, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)\n",
    "import os\n",
    "import deepchem as dc\n",
    "\n",
    "def convert_to_disk_dataset(train_dataset, output_dir):\n",
    "    \"\"\"\n",
    "    Converts a deepchem.data.datasets.NumpyDataset to a deepchem.data.datasets.DiskDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    train_dataset (deepchem.data.datasets.NumpyDataset): The input NumpyDataset to convert.\n",
    "    output_dir (str): The directory to save the DiskDataset.\n",
    "    \n",
    "    Returns:\n",
    "    deepchem.data.datasets.DiskDataset: The converted DiskDataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    X, y, w, ids = train_dataset.X, train_dataset.y, train_dataset.w, train_dataset.ids\n",
    "    \n",
    "    disk_dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids, data_dir=output_dir)\n",
    "    \n",
    "    return disk_dataset\n",
    "\n",
    "output_dir = r'./train_dataset'\n",
    "\n",
    "train_dataset = convert_to_disk_dataset(train_dataset, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratios = get_class_imbalance_ratio(train_dataset)\n",
    "assert len(train_ratios) == n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from deepchem.models.optimizers import ExponentialDecay\n",
    "\n",
    "learning_rate = ExponentialDecay(initial_rate=0.005390333, decay_rate=0.777099289, decay_steps=764, staircase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "nb_epoch = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\feat\\graph_data.py:193: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  g.ndata['x'] = torch.from_numpy(self.node_features).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2.1937479972839355; train_scores = 0.9243541249203545; valid_scores = 0.871121510068558;test_scores = 0.8630193252754054; time_taken = 0:27:05.761146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1314"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = \"./demo_demo_ensemble_models/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model_dmpnn = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                        batch_size=25,\n",
    "                        learning_rate=learning_rate,\n",
    "                        class_imbalance_ratio = train_ratios,\n",
    "                        loss_aggr_type = 'sum',\n",
    "                        node_out_feats = 25,\n",
    "                        edge_hidden_feats = 75,\n",
    "                        edge_out_feats = 25,\n",
    "                        num_step_message_passing = 5,\n",
    "                        mpnn_residual = True,\n",
    "                        message_aggregator_type = 'max',\n",
    "                        mode = 'classification',\n",
    "                        number_atom_features = 133,\n",
    "                        number_bond_features = 14,\n",
    "                        n_classes = 1,\n",
    "                        readout_type = 'set2set',\n",
    "                        num_step_set2set = 3,\n",
    "                        num_layer_set2set = 2,\n",
    "                        ffn_hidden_list= [512, 512],\n",
    "                        ffn_embeddings = 256,\n",
    "                        ffn_activation = 'relu',\n",
    "                        ffn_dropout_p = 0.154493949,\n",
    "                        ffn_dropout_at_input_no_act = False,\n",
    "                        weight_decay = 1.39e-6,\n",
    "                        self_loop = False,\n",
    "                        optimizer_name = 'adam',\n",
    "                        log_frequency = 32,\n",
    "                        model_dir = f'./demo_demo_ensemble_models/experiments_dmpnn',\n",
    "                        device_name='cpu')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "loss = model_dmpnn.fit(\n",
    "        train_dataset,\n",
    "        nb_epoch=nb_epoch,\n",
    "        max_checkpoints_to_keep=1,\n",
    "        deterministic=False,\n",
    "        restore=False)\n",
    "end_time = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_scores = model_dmpnn.evaluate(train_dataset, [metric])['roc_auc_score']\n",
    "valid_scores = model_dmpnn.evaluate(valid_dataset, [metric])['roc_auc_score']\n",
    "test_scores = model_dmpnn.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "print(f\"loss = {loss}; train_scores = {train_scores}; valid_scores = {valid_scores};test_scores = {test_scores}; time_taken = {str(end_time-start_time)}\")\n",
    "model_dmpnn.save_checkpoint() \n",
    "del model_dmpnn\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_preds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:1078: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(checkpoint, map_location=self.device)\n",
      "c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\feat\\graph_data.py:193: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  g.ndata['x'] = torch.from_numpy(self.node_features).float()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_dmpnn = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                        batch_size=25,\n",
    "                        learning_rate=learning_rate,\n",
    "                        class_imbalance_ratio = train_ratios,\n",
    "                        loss_aggr_type = 'sum',\n",
    "                        node_out_feats = 25,\n",
    "                        edge_hidden_feats = 75,\n",
    "                        edge_out_feats = 25,\n",
    "                        num_step_message_passing = 5,\n",
    "                        mpnn_residual = True,\n",
    "                        message_aggregator_type = 'max',\n",
    "                        mode = 'classification',\n",
    "                        number_atom_features = 133,\n",
    "                        number_bond_features = 14,\n",
    "                        n_classes = 1,\n",
    "                        readout_type = 'set2set',\n",
    "                        num_step_set2set = 3,\n",
    "                        num_layer_set2set = 2,\n",
    "                        ffn_hidden_list= [512, 512],\n",
    "                        ffn_embeddings = 256,\n",
    "                        ffn_activation = 'relu', \n",
    "                        ffn_dropout_p = 0.154493949,\n",
    "                        ffn_dropout_at_input_no_act = False,\n",
    "                        weight_decay = 1.39e-6,\n",
    "                        self_loop = False,\n",
    "                        optimizer_name = 'adam',\n",
    "                        log_frequency = 32,\n",
    "                        model_dir = f'./demo_demo_ensemble_models/experiments_dmpnn',\n",
    "                        device_name='cpu')\n",
    "model_dmpnn.restore(f\"./demo_demo_ensemble_models/experiments_dmpnn/checkpoint2.pt\")\n",
    "\n",
    "\n",
    "preds = model_dmpnn.predict(test_dataset)\n",
    "list_preds.append(preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DMPNN Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POM starting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "from openpom.feat.graph_featurizer import GraphFeaturizer, GraphConvConstants\n",
    "from openpom.utils.data_utils import get_class_imbalance_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = GraphFeaturizer()\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "loader = dc.data.CSVLoader(tasks=TASKS,\n",
    "                   feature_field=smiles_field,\n",
    "                   featurizer=featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = loader.create_dataset(inputs=[\"without_O_DMPNNpruned_without hydrogen_curated_GS_LF_merged_4812_QM_cleaned.csv\"])\n",
    "n_tasks = len(dataset.tasks)\n",
    "dataset.X\n",
    "len(dataset.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"without_O_DMPNNpruned_graph_data_cleaned.npz\",allow_pickle=True)\n",
    "mtx_list = data['mtx']\n",
    "qm_graphs = []\n",
    "for mtx in mtx_list:\n",
    "    qm_graphs.append(get_graphs(mtx))\n",
    "QM_X = np.asarray(qm_graphs)\n",
    "QM_X\n",
    "len(QM_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([i for i in zip(dataset.X, QM_X)])\n",
    "new_dataset = dc.data.NumpyDataset(X=X, y=dataset.y, n_tasks=138, ids=dataset.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (4811, 2), y.shape: (4811, 138), w.shape: (4811, 1), task_names: [  0   1   2 ... 135 136 137]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "randomstratifiedsplitter = dc.splits.RandomStratifiedSplitter()\n",
    "train_dataset, test_dataset, valid_dataset = randomstratifiedsplitter.train_valid_test_split(new_dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (3822, 2), y.shape: (3822, 138), w.shape: (3822, 1), task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=train_dataset.X,\n",
    "                    y= train_dataset.y,\n",
    "                    w= train_dataset.w,\n",
    "                    ids=train_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./train_split_pom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (492, 2), y.shape: (492, 138), w.shape: (492, 1), ids: ['CCC(=O)C(=O)O' 'O=C(O)CCCCC(=O)O' 'O=C(O)c1ccccc1' ...\n",
       " 'CCCCCCCCCCC=CCOC(C)=O' 'CCCCCC=CCCC(OCC)OCC' 'CCC1SC(CC(C)C)=NC1C'], task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=valid_dataset.X,\n",
    "                    y= valid_dataset.y,\n",
    "                    w= valid_dataset.w,\n",
    "                    ids=valid_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./valid_split_pom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (497, 2), y.shape: (497, 138), w.shape: (497, 1), ids: ['CC(O)CN' 'O=Cc1ccccc1' 'CC(=O)C(C)=O' ... 'C=C(C)C1CC=C(C2OCC(C)O2)CC1'\n",
       " 'CCCC=CC1OCC(O)CO1' 'CC(C)C1CCC2CC(=O)CC(C)C2(C)C1'], task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=test_dataset.X,\n",
    "                    y= test_dataset.y,\n",
    "                    w= test_dataset.w,\n",
    "                    ids=test_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./test_split_pom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "\n",
    "from deepchem.models.losses import Loss, L2Loss\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from deepchem.models.optimizers import Optimizer, LearningRateSchedule\n",
    "\n",
    "from openpom.layers.pom_ffn import CustomPositionwiseFeedForward\n",
    "from openpom.utils.loss import CustomMultiLabelLoss\n",
    "from openpom.utils.optimizer import get_optimizer\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    from dgl import DGLGraph\n",
    "    from dgl.nn.pytorch import Set2Set\n",
    "    from openpom.layers.pom_mpnn_gnn import CustomMPNNGNN\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    raise ImportError('This module requires dgl and dgllife')\n",
    "\n",
    "\n",
    "class MPNNPOM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'classification',\n",
    "                 number_atom_features: int = 133,\n",
    "                 number_bond_features: int = 14,\n",
    "                 n_classes: int = 1,\n",
    "                 nfeat_name: str = 'x',\n",
    "                 efeat_name: str = 'edge_attr',\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True):\n",
    "\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        super(MPNNPOM, self).__init__()\n",
    "\n",
    "        self.n_tasks: int = n_tasks\n",
    "        self.mode: str = mode\n",
    "        self.n_classes: int = n_classes\n",
    "        self.nfeat_name: str = nfeat_name\n",
    "        self.efeat_name: str = efeat_name\n",
    "        self.readout_type: str = readout_type\n",
    "        self.ffn_embeddings: int = ffn_embeddings\n",
    "        self.ffn_activation: str = ffn_activation\n",
    "        self.ffn_dropout_p: float = ffn_dropout_p\n",
    "\n",
    "        if mode == 'classification':\n",
    "            self.ffn_output: int = n_tasks * n_classes\n",
    "        else:\n",
    "            self.ffn_output = n_tasks\n",
    "\n",
    "        self.mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=number_atom_features,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_in_feats=number_bond_features,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type)\n",
    "        \n",
    "        QM_node_out_features = 50\n",
    "        \n",
    "        self.QM_mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=1,\n",
    "            node_out_feats=QM_node_out_features,\n",
    "            edge_in_feats=1,\n",
    "            edge_hidden_feats=16,\n",
    "            num_step_message_passing=2,\n",
    "            residual=True,\n",
    "            message_aggregator_type='sum')\n",
    "\n",
    "        self.project_edge_feats: nn.Module = nn.Sequential(\n",
    "            nn.Linear(number_bond_features, edge_out_feats), nn.ReLU())\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            self.readout_set2set: nn.Module = Set2Set(\n",
    "                input_dim=node_out_feats + edge_out_feats,\n",
    "                n_iters=num_step_set2set,\n",
    "                n_layers=num_layer_set2set)\n",
    "            ffn_input: int = 2 * (node_out_feats + edge_out_feats)\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            ffn_input = node_out_feats + edge_out_feats\n",
    "        else:\n",
    "            raise Exception(\"readout_type invalid\")\n",
    "\n",
    "        if ffn_embeddings is not None:\n",
    "            d_hidden_list: List = ffn_hidden_list + [ffn_embeddings]\n",
    "\n",
    "        self.ffn: nn.Module = CustomPositionwiseFeedForward(\n",
    "            d_input=ffn_input + QM_node_out_features,\n",
    "            \n",
    "            d_hidden_list=d_hidden_list,\n",
    "            d_output=self.ffn_output,\n",
    "            activation=ffn_activation,\n",
    "            dropout_p=ffn_dropout_p,\n",
    "            dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "    def _readout(self, g: DGLGraph, node_encodings: torch.Tensor,\n",
    "                 edge_feats: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        g.ndata['node_emb'] = node_encodings\n",
    "        g.edata['edge_emb'] = self.project_edge_feats(edge_feats)\n",
    "\n",
    "        def message_func(edges) -> Dict:\n",
    "            \"\"\"\n",
    "            The message function to generate messages\n",
    "            along the edges for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg: torch.Tensor = torch.cat(\n",
    "                (edges.src['node_emb'], edges.data['edge_emb']), dim=1)\n",
    "            return {'src_msg': src_msg}\n",
    "\n",
    "        def reduce_func(nodes) -> Dict:\n",
    "            \"\"\"\n",
    "            The reduce function to aggregate the messages\n",
    "            for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg_sum: torch.Tensor = torch.sum(nodes.mailbox['src_msg'],\n",
    "                                                  dim=1)\n",
    "            return {'src_msg_sum': src_msg_sum}\n",
    "\n",
    "        \n",
    "        g.send_and_recv(g.edges(),\n",
    "                        message_func=message_func,\n",
    "                        reduce_func=reduce_func)\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            batch_mol_hidden_states: torch.Tensor = self.readout_set2set(\n",
    "                g, g.ndata['src_msg_sum'])\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            batch_mol_hidden_states = dgl.sum_nodes(g, 'src_msg_sum')\n",
    "\n",
    "        \n",
    "        return batch_mol_hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self, graphs: tuple[DGLGraph]\n",
    "    ) -> Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        g, qm_g = graphs\n",
    "        node_feats: torch.Tensor = g.ndata[self.nfeat_name]\n",
    "        edge_feats: torch.Tensor = g.edata[self.efeat_name]\n",
    "\n",
    "        qm_node_feats: torch.Tensor = qm_g.ndata[self.nfeat_name]\n",
    "        qm_edge_feats: torch.Tensor = qm_g.edata[self.efeat_name]\n",
    "\n",
    "        node_encodings: torch.Tensor = self.mpnn(g, node_feats, edge_feats)\n",
    "\n",
    "        QM_encodings: torch.Tensor = self.QM_mpnn(qm_g, qm_node_feats, qm_edge_feats)\n",
    "        qm_g.ndata['node_emb'] = QM_encodings\n",
    "        molecular_QM_encodings = dgl.sum_nodes(qm_g, 'node_emb')\n",
    "        molecular_encodings: torch.Tensor = self._readout(\n",
    "            g, node_encodings, edge_feats)\n",
    "        if self.readout_type == 'global_sum_pooling':\n",
    "            molecular_encodings = F.softmax(molecular_encodings, dim=1)\n",
    "\n",
    "        embeddings: torch.Tensor\n",
    "        out: torch.Tensor\n",
    "        embeddings, out = self.ffn(torch.concat((molecular_encodings, molecular_QM_encodings), dim=1))\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            if self.n_tasks == 1:\n",
    "                logits: torch.Tensor = out.view(-1, self.n_classes)\n",
    "            else:\n",
    "                logits = out.view(-1, self.n_tasks, self.n_classes)\n",
    "            proba: torch.Tensor = F.sigmoid(\n",
    "                logits)  \n",
    "            if self.n_classes == 1:\n",
    "                proba = proba.squeeze(-1)  \n",
    "            return proba, logits, embeddings\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class MPNNPOMModel(TorchModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 class_imbalance_ratio: Optional[List] = None,\n",
    "                 loss_aggr_type: str = 'sum',\n",
    "                 learning_rate: Union[float, LearningRateSchedule] = 0.001,\n",
    "                 batch_size: int = 25,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'regression',\n",
    "                 number_atom_features: int = 134,\n",
    "                 number_bond_features: int = 6,\n",
    "                 n_classes: int = 1,\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True,\n",
    "                 weight_decay: float = 1e-5,\n",
    "                 self_loop: bool = False,\n",
    "                 optimizer_name: str = 'adam',\n",
    "                 device_name: Optional[str] = None,\n",
    "                 **kwargs):\n",
    "        model: nn.Module = MPNNPOM(\n",
    "            n_tasks=n_tasks,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            edge_out_feats=edge_out_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            mpnn_residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type,\n",
    "            mode=mode,\n",
    "            number_atom_features=number_atom_features,\n",
    "            number_bond_features=number_bond_features,\n",
    "            n_classes=n_classes,\n",
    "            readout_type=readout_type,\n",
    "            num_step_set2set=num_step_set2set,\n",
    "            num_layer_set2set=num_layer_set2set,\n",
    "            ffn_hidden_list=ffn_hidden_list,\n",
    "            ffn_embeddings=ffn_embeddings,\n",
    "            ffn_activation=ffn_activation,\n",
    "            ffn_dropout_p=ffn_dropout_p,\n",
    "            ffn_dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "        if class_imbalance_ratio and (len(class_imbalance_ratio) != n_tasks):\n",
    "            raise Exception(\"size of class_imbalance_ratio \\\n",
    "                            should be equal to n_tasks\")\n",
    "\n",
    "        if mode == 'regression':\n",
    "            loss: Loss = L2Loss()\n",
    "            output_types: List = ['prediction']\n",
    "        else:\n",
    "            loss = CustomMultiLabelLoss(\n",
    "                class_imbalance_ratio=class_imbalance_ratio,\n",
    "                loss_aggr_type=loss_aggr_type,\n",
    "                device=device_name)\n",
    "            output_types = ['prediction', 'loss', 'embedding']\n",
    "\n",
    "        optimizer: Optimizer = get_optimizer(optimizer_name)\n",
    "        optimizer.learning_rate = learning_rate\n",
    "        if device_name is not None:\n",
    "            device: Optional[torch.device] = torch.device(device_name)\n",
    "        else:\n",
    "            device = None\n",
    "        super(MPNNPOMModel, self).__init__(model,\n",
    "                                           loss=loss,\n",
    "                                           output_types=output_types,\n",
    "                                           optimizer=optimizer,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           device=device,\n",
    "                                           **kwargs)\n",
    "\n",
    "        self.weight_decay: float = weight_decay\n",
    "        self._self_loop: bool = self_loop\n",
    "        self.regularization_loss: Callable = self._regularization_loss\n",
    "\n",
    "    def _regularization_loss(self) -> torch.Tensor:\n",
    "        l1_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        l2_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                l1_regularization = l1_regularization + torch.norm(param, p=1)\n",
    "                l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
    "        l1_norm: torch.Tensor = self.weight_decay * l1_regularization\n",
    "        l2_norm: torch.Tensor = self.weight_decay * l2_regularization\n",
    "        return l1_norm + l2_norm\n",
    "\n",
    "    def _prepare_batch(\n",
    "        self, batch: Tuple[List, List, List]\n",
    "    ) -> Tuple[DGLGraph, List[torch.Tensor], List[torch.Tensor]]:\n",
    "        inputs: List\n",
    "        labels: List\n",
    "        weights: List\n",
    "\n",
    "        inputs, labels, weights = batch\n",
    "        dgl_graphs1: List[DGLGraph] = [\n",
    "            graphs[0].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graphs in inputs[0]\n",
    "        ]\n",
    "        g1: DGLGraph = dgl.batch(dgl_graphs1).to(self.device)\n",
    "\n",
    "        dgl_graphs2: List[DGLGraph] = [\n",
    "            graphs[1].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graphs in inputs[0]\n",
    "        ]\n",
    "        g2: DGLGraph = dgl.batch(dgl_graphs2).to(self.device)\n",
    "        _, labels, weights = super(MPNNPOMModel, self)._prepare_batch(\n",
    "            ([], labels, weights))\n",
    "        return (g1, g2), labels, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)\n",
    "import os\n",
    "import deepchem as dc\n",
    "\n",
    "def convert_to_disk_dataset(train_dataset, output_dir):\n",
    "    \"\"\"\n",
    "    Converts a deepchem.data.datasets.NumpyDataset to a deepchem.data.datasets.DiskDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    train_dataset (deepchem.data.datasets.NumpyDataset): The input NumpyDataset to convert.\n",
    "    output_dir (str): The directory to save the DiskDataset.\n",
    "    \n",
    "    Returns:\n",
    "    deepchem.data.datasets.DiskDataset: The converted DiskDataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    X, y, w, ids = train_dataset.X, train_dataset.y, train_dataset.w, train_dataset.ids\n",
    "    \n",
    "    disk_dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids, data_dir=output_dir)\n",
    "    \n",
    "    return disk_dataset\n",
    "output_dir = './train_dataset'\n",
    "train_dataset = convert_to_disk_dataset(train_dataset, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratios = get_class_imbalance_ratio(train_dataset)\n",
    "assert len(train_ratios) == n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.optimizers import ExponentialDecay\n",
    "\n",
    "learning_rate = ExponentialDecay(initial_rate=0.005390333, decay_rate=0.777099289, decay_steps=764, staircase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "nb_epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2.2464890480041504; train_scores = 0.9160328203836063; valid_scores = 0.8677378734560544;test_scores = 0.8605574699630371; time_taken = 0:27:01.574006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1212"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = \"./demo_demo_ensemble_models/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model_pom = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                            batch_size=25,\n",
    "                            learning_rate=learning_rate,\n",
    "                            class_imbalance_ratio = train_ratios,\n",
    "                            loss_aggr_type = 'sum',\n",
    "                            node_out_feats = 25,\n",
    "                            edge_hidden_feats = 75,\n",
    "                            edge_out_feats = 25,\n",
    "                            num_step_message_passing = 5,\n",
    "                            mpnn_residual = True,\n",
    "                            message_aggregator_type = 'max',\n",
    "                            mode = 'classification',\n",
    "                            number_atom_features = 134,\n",
    "                            number_bond_features = 6,\n",
    "                            n_classes = 1,\n",
    "                            readout_type = 'set2set',\n",
    "                            num_step_set2set = 3,\n",
    "                            num_layer_set2set = 2,\n",
    "                            ffn_hidden_list= [512, 512],\n",
    "                            ffn_embeddings = 256,\n",
    "                            ffn_activation = 'relu',\n",
    "                            ffn_dropout_p = 0.154493949,\n",
    "                            ffn_dropout_at_input_no_act = False,\n",
    "                            weight_decay = 1.39e-6,\n",
    "                            self_loop = False,\n",
    "                            optimizer_name = 'adam',\n",
    "                            log_frequency = 32,\n",
    "                            model_dir = f'./demo_demo_ensemble_models/experiments_pom',\n",
    "                            device_name='cpu')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "loss = model_pom.fit(\n",
    "        train_dataset,\n",
    "        nb_epoch=nb_epoch,\n",
    "        max_checkpoints_to_keep=1,\n",
    "        deterministic=False,\n",
    "        restore=False)\n",
    "end_time = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_scores = model_pom.evaluate(train_dataset, [metric])['roc_auc_score']\n",
    "valid_scores = model_pom.evaluate(valid_dataset, [metric])['roc_auc_score']\n",
    "test_scores = model_pom.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "print(f\"loss = {loss}; train_scores = {train_scores}; valid_scores = {valid_scores};test_scores = {test_scores}; time_taken = {str(end_time-start_time)}\")\n",
    "model_pom.save_checkpoint() \n",
    "del model_pom \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:1078: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(checkpoint, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "model_pom = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                            batch_size=25,\n",
    "                            learning_rate=learning_rate,\n",
    "                            class_imbalance_ratio = train_ratios,\n",
    "                            loss_aggr_type = 'sum',\n",
    "                            node_out_feats = 25,\n",
    "                            edge_hidden_feats = 75,\n",
    "                            edge_out_feats = 25,\n",
    "                            num_step_message_passing = 5,\n",
    "                            mpnn_residual = True,\n",
    "                            message_aggregator_type = 'max',\n",
    "                            mode = 'classification',\n",
    "                            number_atom_features = 134,\n",
    "                            number_bond_features = 6,\n",
    "                            n_classes = 1,\n",
    "                            readout_type = 'set2set',\n",
    "                            num_step_set2set = 3,\n",
    "                            num_layer_set2set = 2,\n",
    "                            ffn_hidden_list= [512, 512],\n",
    "                            ffn_embeddings = 256,\n",
    "                            ffn_activation = 'relu',\n",
    "                            ffn_dropout_p = 0.154493949,\n",
    "                            ffn_dropout_at_input_no_act = False,\n",
    "                            weight_decay = 1.39e-6,\n",
    "                            self_loop = False,\n",
    "                            optimizer_name = 'adam',\n",
    "                            log_frequency = 32,\n",
    "                            model_dir = f'./demo_demo_ensemble_models/experiments_pom',\n",
    "                            device_name='cpu')\n",
    "model_pom.restore(f\"./demo_demo_ensemble_models/experiments_pom/checkpoint2.pt\")\n",
    "\n",
    "\n",
    "preds = model_pom.predict(test_dataset)\n",
    "list_preds.append(preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POM complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.feat import MolGraphConvFeaturizer\n",
    "import deepchem as dc\n",
    "\n",
    "\n",
    "featurizer = MolGraphConvFeaturizer(use_edges=True)\n",
    "\n",
    "smiles_field = 'nonStereoSMILES'\n",
    "\n",
    "\n",
    "loader = dc.data.CSVLoader(\n",
    "    tasks=TASKS,  \n",
    "    feature_field=smiles_field,\n",
    "    featurizer=featurizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = loader.create_dataset(inputs=[\"without_O_DMPNNpruned_without hydrogen_curated_GS_LF_merged_4812_QM_cleaned.csv\"])\n",
    "n_tasks = len(dataset.tasks)\n",
    "dataset.X\n",
    "len(dataset.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4811"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(\"without_O_DMPNNpruned_graph_data_cleaned.npz\",allow_pickle=True)\n",
    "mtx_list = data['mtx']\n",
    "qm_graphs = []\n",
    "for mtx in mtx_list:\n",
    "    qm_graphs.append(get_graphs(mtx))\n",
    "QM_X = np.asarray(qm_graphs)\n",
    "QM_X\n",
    "len(QM_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([i for i in zip(dataset.X, QM_X)])\n",
    "new_dataset = dc.data.NumpyDataset(X=X, y=dataset.y, n_tasks=138, ids=dataset.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NumpyDataset X.shape: (4811, 2), y.shape: (4811, 138), w.shape: (4811, 1), task_names: [  0   1   2 ... 135 136 137]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "randomstratifiedsplitter = dc.splits.RandomStratifiedSplitter()\n",
    "train_dataset, test_dataset, valid_dataset = randomstratifiedsplitter.train_valid_test_split(new_dataset, frac_train = 0.8, frac_valid = 0.1, frac_test = 0.1, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (3822, 2), y.shape: (3822, 138), w.shape: (3822, 1), task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=train_dataset.X,\n",
    "                    y= train_dataset.y,\n",
    "                    w= train_dataset.w,\n",
    "                    ids=train_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./train_split_gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (492, 2), y.shape: (492, 138), w.shape: (492, 1), ids: ['CCC(=O)C(=O)O' 'O=C(O)CCCCC(=O)O' 'O=C(O)c1ccccc1' ...\n",
       " 'CCCCCCCCCCC=CCOC(C)=O' 'CCCCCC=CCCC(OCC)OCC' 'CCC1SC(CC(C)C)=NC1C'], task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=valid_dataset.X,\n",
    "                    y= valid_dataset.y,\n",
    "                    w= valid_dataset.w,\n",
    "                    ids=valid_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./valid_split_gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (497, 2), y.shape: (497, 138), w.shape: (497, 1), ids: ['CC(O)CN' 'O=Cc1ccccc1' 'CC(=O)C(C)=O' ... 'C=C(C)C1CC=C(C2OCC(C)O2)CC1'\n",
       " 'CCCC=CC1OCC(O)CO1' 'CC(C)C1CCC2CC(=O)CC(C)C2(C)C1'], task_names: ['alcoholic' 'aldehydic' 'alliaceous' ... 'weedy' 'winey' 'woody']>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc.data.DiskDataset.from_numpy(X=test_dataset.X,\n",
    "                    y= test_dataset.y,\n",
    "                    w= test_dataset.w,\n",
    "                    ids=test_dataset.ids,\n",
    "                    tasks=TASKS,\n",
    "                    data_dir=\"./test_split_gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple, Union, Optional, Callable, Dict\n",
    "\n",
    "from deepchem.models.losses import Loss, L2Loss\n",
    "from deepchem.models.torch_models.torch_model import TorchModel\n",
    "from deepchem.models.optimizers import Optimizer, LearningRateSchedule\n",
    "\n",
    "from openpom.layers.pom_ffn import CustomPositionwiseFeedForward\n",
    "from openpom.utils.loss import CustomMultiLabelLoss\n",
    "from openpom.utils.optimizer import get_optimizer\n",
    "\n",
    "try:\n",
    "    import dgl\n",
    "    from dgl import DGLGraph\n",
    "    from dgl.nn.pytorch import Set2Set\n",
    "    from openpom.layers.pom_mpnn_gnn import CustomMPNNGNN\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    raise ImportError('This module requires dgl and dgllife')\n",
    "\n",
    "\n",
    "class MPNNPOM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'classification',\n",
    "                 number_atom_features: int = 133,\n",
    "                 number_bond_features: int = 14,\n",
    "                 n_classes: int = 1,\n",
    "                 nfeat_name: str = 'x',\n",
    "                 efeat_name: str = 'edge_attr',\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True):\n",
    "\n",
    "        if mode not in ['classification', 'regression']:\n",
    "            raise ValueError(\n",
    "                \"mode must be either 'classification' or 'regression'\")\n",
    "\n",
    "        super(MPNNPOM, self).__init__()\n",
    "\n",
    "        self.n_tasks: int = n_tasks\n",
    "        self.mode: str = mode\n",
    "        self.n_classes: int = n_classes\n",
    "        self.nfeat_name: str = nfeat_name\n",
    "        self.efeat_name: str = efeat_name\n",
    "        self.readout_type: str = readout_type\n",
    "        self.ffn_embeddings: int = ffn_embeddings\n",
    "        self.ffn_activation: str = ffn_activation\n",
    "        self.ffn_dropout_p: float = ffn_dropout_p\n",
    "\n",
    "        if mode == 'classification':\n",
    "            self.ffn_output: int = n_tasks * n_classes\n",
    "        else:\n",
    "            self.ffn_output = n_tasks\n",
    "\n",
    "        self.mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=number_atom_features,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_in_feats=number_bond_features,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type)\n",
    "        \n",
    "        QM_node_out_features = 50\n",
    "        \n",
    "        self.QM_mpnn: nn.Module = CustomMPNNGNN(\n",
    "            node_in_feats=1,\n",
    "            node_out_feats=QM_node_out_features,\n",
    "            edge_in_feats=1,\n",
    "            edge_hidden_feats=16,\n",
    "            num_step_message_passing=2,\n",
    "            residual=True,\n",
    "            message_aggregator_type='sum')\n",
    "\n",
    "        self.project_edge_feats: nn.Module = nn.Sequential(\n",
    "            nn.Linear(number_bond_features, edge_out_feats), nn.ReLU())\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            self.readout_set2set: nn.Module = Set2Set(\n",
    "                input_dim=node_out_feats + edge_out_feats,\n",
    "                n_iters=num_step_set2set,\n",
    "                n_layers=num_layer_set2set)\n",
    "            ffn_input: int = 2 * (node_out_feats + edge_out_feats)\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            ffn_input = node_out_feats + edge_out_feats\n",
    "        else:\n",
    "            raise Exception(\"readout_type invalid\")\n",
    "\n",
    "        if ffn_embeddings is not None:\n",
    "            d_hidden_list: List = ffn_hidden_list + [ffn_embeddings]\n",
    "\n",
    "        self.ffn: nn.Module = CustomPositionwiseFeedForward(\n",
    "            d_input=ffn_input + QM_node_out_features,\n",
    "            \n",
    "            d_hidden_list=d_hidden_list,\n",
    "            d_output=self.ffn_output,\n",
    "            activation=ffn_activation,\n",
    "            dropout_p=ffn_dropout_p,\n",
    "            dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "    def _readout(self, g: DGLGraph, node_encodings: torch.Tensor,\n",
    "                 edge_feats: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        g.ndata['node_emb'] = node_encodings\n",
    "        g.edata['edge_emb'] = self.project_edge_feats(edge_feats)\n",
    "\n",
    "        def message_func(edges) -> Dict:\n",
    "            \"\"\"\n",
    "            The message function to generate messages\n",
    "            along the edges for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg: torch.Tensor = torch.cat(\n",
    "                (edges.src['node_emb'], edges.data['edge_emb']), dim=1)\n",
    "            return {'src_msg': src_msg}\n",
    "\n",
    "        def reduce_func(nodes) -> Dict:\n",
    "            \"\"\"\n",
    "            The reduce function to aggregate the messages\n",
    "            for DGLGraph.send_and_recv()\n",
    "            \"\"\"\n",
    "            src_msg_sum: torch.Tensor = torch.sum(nodes.mailbox['src_msg'],\n",
    "                                                  dim=1)\n",
    "            return {'src_msg_sum': src_msg_sum}\n",
    "\n",
    "        \n",
    "        g.send_and_recv(g.edges(),\n",
    "                        message_func=message_func,\n",
    "                        reduce_func=reduce_func)\n",
    "\n",
    "        if self.readout_type == 'set2set':\n",
    "            batch_mol_hidden_states: torch.Tensor = self.readout_set2set(\n",
    "                g, g.ndata['src_msg_sum'])\n",
    "        elif self.readout_type == 'global_sum_pooling':\n",
    "            batch_mol_hidden_states = dgl.sum_nodes(g, 'src_msg_sum')\n",
    "\n",
    "        \n",
    "        return batch_mol_hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self, graphs: tuple[DGLGraph]\n",
    "    ) -> Union[tuple[torch.Tensor, torch.Tensor, torch.Tensor], torch.Tensor]:\n",
    "        g, qm_g = graphs\n",
    "        node_feats: torch.Tensor = g.ndata[self.nfeat_name]\n",
    "        edge_feats: torch.Tensor = g.edata[self.efeat_name]\n",
    "\n",
    "        qm_node_feats: torch.Tensor = qm_g.ndata[self.nfeat_name]\n",
    "        qm_edge_feats: torch.Tensor = qm_g.edata[self.efeat_name]\n",
    "\n",
    "        node_encodings: torch.Tensor = self.mpnn(g, node_feats, edge_feats)\n",
    "\n",
    "        QM_encodings: torch.Tensor = self.QM_mpnn(qm_g, qm_node_feats, qm_edge_feats)\n",
    "        qm_g.ndata['node_emb'] = QM_encodings\n",
    "        molecular_QM_encodings = dgl.sum_nodes(qm_g, 'node_emb')\n",
    "        molecular_encodings: torch.Tensor = self._readout(\n",
    "            g, node_encodings, edge_feats)\n",
    "        if self.readout_type == 'global_sum_pooling':\n",
    "            molecular_encodings = F.softmax(molecular_encodings, dim=1)\n",
    "\n",
    "        embeddings: torch.Tensor\n",
    "        out: torch.Tensor\n",
    "        embeddings, out = self.ffn(torch.concat((molecular_encodings, molecular_QM_encodings), dim=1))\n",
    "        \n",
    "        \n",
    "\n",
    "        if self.mode == 'classification':\n",
    "            if self.n_tasks == 1:\n",
    "                logits: torch.Tensor = out.view(-1, self.n_classes)\n",
    "            else:\n",
    "                logits = out.view(-1, self.n_tasks, self.n_classes)\n",
    "            proba: torch.Tensor = F.sigmoid(\n",
    "                logits)  \n",
    "            if self.n_classes == 1:\n",
    "                proba = proba.squeeze(-1)  \n",
    "            return proba, logits, embeddings\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "\n",
    "class MPNNPOMModel(TorchModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_tasks: int,\n",
    "                 class_imbalance_ratio: Optional[List] = None,\n",
    "                 loss_aggr_type: str = 'sum',\n",
    "                 learning_rate: Union[float, LearningRateSchedule] = 0.001,\n",
    "                 batch_size: int = 25,\n",
    "                 node_out_feats: int = 64,\n",
    "                 edge_hidden_feats: int = 128,\n",
    "                 edge_out_feats: int = 64,\n",
    "                 num_step_message_passing: int = 3,\n",
    "                 mpnn_residual: bool = True,\n",
    "                 message_aggregator_type: str = 'sum',\n",
    "                 mode: str = 'regression',\n",
    "                 number_atom_features: int = 30,\n",
    "                 number_bond_features: int = 11,\n",
    "                 n_classes: int = 1,\n",
    "                 readout_type: str = 'set2set',\n",
    "                 num_step_set2set: int = 6,\n",
    "                 num_layer_set2set: int = 3,\n",
    "                 ffn_hidden_list: List = [300],\n",
    "                 ffn_embeddings: int = 256,\n",
    "                 ffn_activation: str = 'relu',\n",
    "                 ffn_dropout_p: float = 0.0,\n",
    "                 ffn_dropout_at_input_no_act: bool = True,\n",
    "                 weight_decay: float = 1e-5,\n",
    "                 self_loop: bool = False,\n",
    "                 optimizer_name: str = 'adam',\n",
    "                 device_name: Optional[str] = None,\n",
    "                 **kwargs):\n",
    "        model: nn.Module = MPNNPOM(\n",
    "            n_tasks=n_tasks,\n",
    "            node_out_feats=node_out_feats,\n",
    "            edge_hidden_feats=edge_hidden_feats,\n",
    "            edge_out_feats=edge_out_feats,\n",
    "            num_step_message_passing=num_step_message_passing,\n",
    "            mpnn_residual=mpnn_residual,\n",
    "            message_aggregator_type=message_aggregator_type,\n",
    "            mode=mode,\n",
    "            number_atom_features=number_atom_features,\n",
    "            number_bond_features=number_bond_features,\n",
    "            n_classes=n_classes,\n",
    "            readout_type=readout_type,\n",
    "            num_step_set2set=num_step_set2set,\n",
    "            num_layer_set2set=num_layer_set2set,\n",
    "            ffn_hidden_list=ffn_hidden_list,\n",
    "            ffn_embeddings=ffn_embeddings,\n",
    "            ffn_activation=ffn_activation,\n",
    "            ffn_dropout_p=ffn_dropout_p,\n",
    "            ffn_dropout_at_input_no_act=ffn_dropout_at_input_no_act)\n",
    "\n",
    "        if class_imbalance_ratio and (len(class_imbalance_ratio) != n_tasks):\n",
    "            raise Exception(\"size of class_imbalance_ratio \\\n",
    "                            should be equal to n_tasks\")\n",
    "\n",
    "        if mode == 'regression':\n",
    "            loss: Loss = L2Loss()\n",
    "            output_types: List = ['prediction']\n",
    "        else:\n",
    "            loss = CustomMultiLabelLoss(\n",
    "                class_imbalance_ratio=class_imbalance_ratio,\n",
    "                loss_aggr_type=loss_aggr_type,\n",
    "                device=device_name)\n",
    "            output_types = ['prediction', 'loss', 'embedding']\n",
    "\n",
    "        optimizer: Optimizer = get_optimizer(optimizer_name)\n",
    "        optimizer.learning_rate = learning_rate\n",
    "        if device_name is not None:\n",
    "            device: Optional[torch.device] = torch.device(device_name)\n",
    "        else:\n",
    "            device = None\n",
    "        super(MPNNPOMModel, self).__init__(model,\n",
    "                                           loss=loss,\n",
    "                                           output_types=output_types,\n",
    "                                           optimizer=optimizer,\n",
    "                                           learning_rate=learning_rate,\n",
    "                                           batch_size=batch_size,\n",
    "                                           device=device,\n",
    "                                           **kwargs)\n",
    "\n",
    "        self.weight_decay: float = weight_decay\n",
    "        self._self_loop: bool = self_loop\n",
    "        self.regularization_loss: Callable = self._regularization_loss\n",
    "\n",
    "    def _regularization_loss(self) -> torch.Tensor:\n",
    "        l1_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        l2_regularization: torch.Tensor = torch.tensor(0., requires_grad=True)\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                l1_regularization = l1_regularization + torch.norm(param, p=1)\n",
    "                l2_regularization = l2_regularization + torch.norm(param, p=2)\n",
    "        l1_norm: torch.Tensor = self.weight_decay * l1_regularization\n",
    "        l2_norm: torch.Tensor = self.weight_decay * l2_regularization\n",
    "        return l1_norm + l2_norm\n",
    "\n",
    "    def _prepare_batch(\n",
    "        self, batch: Tuple[List, List, List]\n",
    "    ) -> Tuple[DGLGraph, List[torch.Tensor], List[torch.Tensor]]:\n",
    "        inputs: List\n",
    "        labels: List\n",
    "        weights: List\n",
    "\n",
    "        inputs, labels, weights = batch\n",
    "        dgl_graphs1: List[DGLGraph] = [\n",
    "            graphs[0].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graphs in inputs[0]\n",
    "        ]\n",
    "        g1: DGLGraph = dgl.batch(dgl_graphs1).to(self.device)\n",
    "\n",
    "        dgl_graphs2: List[DGLGraph] = [\n",
    "            graphs[1].to_dgl_graph(self_loop=self._self_loop)\n",
    "            for graphs in inputs[0]\n",
    "        ]\n",
    "        g2: DGLGraph = dgl.batch(dgl_graphs2).to(self.device)\n",
    "        _, labels, weights = super(MPNNPOMModel, self)._prepare_batch(\n",
    "            ([], labels, weights))\n",
    "        return (g1, g2), labels, weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_dataset)\n",
    "import os\n",
    "import deepchem as dc\n",
    "\n",
    "def convert_to_disk_dataset(train_dataset, output_dir):\n",
    "    \"\"\"\n",
    "    Converts a deepchem.data.datasets.NumpyDataset to a deepchem.data.datasets.DiskDataset.\n",
    "    \n",
    "    Parameters:\n",
    "    train_dataset (deepchem.data.datasets.NumpyDataset): The input NumpyDataset to convert.\n",
    "    output_dir (str): The directory to save the DiskDataset.\n",
    "    \n",
    "    Returns:\n",
    "    deepchem.data.datasets.DiskDataset: The converted DiskDataset.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    X, y, w, ids = train_dataset.X, train_dataset.y, train_dataset.w, train_dataset.ids\n",
    "    \n",
    "    disk_dataset = dc.data.DiskDataset.from_numpy(X, y, w, ids, data_dir=output_dir)\n",
    "    \n",
    "    return disk_dataset\n",
    "output_dir = './train_dataset'\n",
    "train_dataset = convert_to_disk_dataset(train_dataset, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratios = get_class_imbalance_ratio(train_dataset)\n",
    "assert len(train_ratios) == n_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchem.models.optimizers import ExponentialDecay\n",
    "\n",
    "learning_rate = ExponentialDecay(initial_rate=0.005390333, decay_rate=0.777099289, decay_steps=764, staircase=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "nb_epoch = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 2.1694514751434326; train_scores = 0.9290333876894046; valid_scores = 0.8686874035674178;test_scores = 0.8650435654197672; time_taken = 0:26:33.282115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1176"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir = \"./demo_demo_ensemble_models/\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "model_gcn = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                            batch_size=25,\n",
    "                            learning_rate=learning_rate,\n",
    "                            class_imbalance_ratio = train_ratios,\n",
    "                            loss_aggr_type = 'sum',\n",
    "                            node_out_feats = 25,\n",
    "                            edge_hidden_feats = 75,\n",
    "                            edge_out_feats = 25,\n",
    "                            num_step_message_passing = 5,\n",
    "                            mpnn_residual = True,\n",
    "                            message_aggregator_type = 'max',\n",
    "                            mode = 'classification',\n",
    "                            number_atom_features = 30,\n",
    "                            number_bond_features = 11,\n",
    "                            n_classes = 1,\n",
    "                            readout_type = 'set2set',\n",
    "                            num_step_set2set = 3,\n",
    "                            num_layer_set2set = 2,\n",
    "                            ffn_hidden_list= [512, 512],\n",
    "                            ffn_embeddings = 256,\n",
    "                            ffn_activation = 'relu',\n",
    "                            ffn_dropout_p = 0.154493949,\n",
    "                            ffn_dropout_at_input_no_act = False,\n",
    "                            weight_decay = 1.39e-6,\n",
    "                            self_loop = False,\n",
    "                            optimizer_name = 'adam',\n",
    "                            log_frequency = 32,\n",
    "                            model_dir = f'./demo_demo_ensemble_models/experiments_gcn',\n",
    "                            device_name='cpu')\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "\n",
    "loss = model_gcn.fit(\n",
    "        train_dataset,\n",
    "        nb_epoch=nb_epoch,\n",
    "        max_checkpoints_to_keep=1,\n",
    "        deterministic=False,\n",
    "        restore=False)\n",
    "end_time = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_scores =  model_gcn.evaluate(train_dataset, [metric])['roc_auc_score']\n",
    "valid_scores =  model_gcn.evaluate(valid_dataset, [metric])['roc_auc_score']\n",
    "test_scores =  model_gcn.evaluate(test_dataset, [metric])['roc_auc_score']\n",
    "print(f\"loss = {loss}; train_scores = {train_scores}; valid_scores = {valid_scores};test_scores = {test_scores}; time_taken = {str(end_time-start_time)}\")\n",
    "model_gcn.save_checkpoint() \n",
    "del  model_gcn \n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\miniconda3\\envs\\ENV4\\lib\\site-packages\\deepchem\\models\\torch_models\\torch_model.py:1078: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(checkpoint, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "model_gcn = MPNNPOMModel(n_tasks = n_tasks,\n",
    "                            batch_size=25,\n",
    "                            learning_rate=learning_rate,\n",
    "                            class_imbalance_ratio = train_ratios,\n",
    "                            loss_aggr_type = 'sum',\n",
    "                            node_out_feats = 25,\n",
    "                            edge_hidden_feats = 75,\n",
    "                            edge_out_feats = 25,\n",
    "                            num_step_message_passing = 5,\n",
    "                            mpnn_residual = True,\n",
    "                            message_aggregator_type = 'max',\n",
    "                            mode = 'classification',\n",
    "                            number_atom_features = 30,\n",
    "                            number_bond_features = 11,\n",
    "                            n_classes = 1,\n",
    "                            readout_type = 'set2set',\n",
    "                            num_step_set2set = 3,\n",
    "                            num_layer_set2set = 2,\n",
    "                            ffn_hidden_list= [512, 512],\n",
    "                            ffn_embeddings = 256,\n",
    "                            ffn_activation = 'relu',\n",
    "                            ffn_dropout_p = 0.154493949,\n",
    "                            ffn_dropout_at_input_no_act = False,\n",
    "                            weight_decay = 1.39e-6,\n",
    "                            self_loop = False,\n",
    "                            optimizer_name = 'adam',\n",
    "                            log_frequency = 32,\n",
    "                            model_dir = f'./demo_demo_ensemble_models/experiments_gcn',\n",
    "                            device_name='cpu')\n",
    "model_gcn.restore(f\"./demo_demo_ensemble_models/experiments_gcn/checkpoint2.pt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The below code will run only in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test ensemble score:  0.8650432975117219\n",
      "Mean Valid ensemble score:  0.8686874035674178\n"
     ]
    }
   ],
   "source": [
    "list_preds= []\n",
    "preds = model_gcn.predict(test_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.mean(preds_arr, axis=0)\n",
    "print(\"Mean Test ensemble score: \", roc_auc_score(test_dataset.y, ensemble_preds, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "list_preds = []\n",
    "preds = model_gcn.predict(valid_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.mean(preds_arr, axis=0)\n",
    "print(\"Mean Valid ensemble score: \", roc_auc_score(valid_dataset.y, ensemble_preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Test ensemble score:  0.8650432975117219\n",
      "Max Valid ensemble score:  0.8686874035674178\n"
     ]
    }
   ],
   "source": [
    "list_preds= []\n",
    "preds = model_gcn.predict(test_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.max(preds_arr, axis=0)\n",
    "print(\"Max Test ensemble score: \", roc_auc_score(test_dataset.y, ensemble_preds, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "list_preds = []\n",
    "preds = model_gcn.predict(valid_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.max(preds_arr, axis=0)\n",
    "print(\"Max Valid ensemble score: \", roc_auc_score(valid_dataset.y, ensemble_preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Test ensemble score:  0.8650432975117219\n",
      "Min Valid ensemble score:  0.8686874035674178\n"
     ]
    }
   ],
   "source": [
    "list_preds= []\n",
    "preds = model_gcn.predict(test_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.min(preds_arr, axis=0)\n",
    "print(\"Min Test ensemble score: \", roc_auc_score(test_dataset.y, ensemble_preds, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "list_preds = []\n",
    "preds = model_gcn.predict(valid_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds) \n",
    "ensemble_preds = np.min(preds_arr, axis=0)\n",
    "print(\"Min Valid ensemble score: \", roc_auc_score(valid_dataset.y, ensemble_preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Test ensemble score:  0.8650432975117219\n",
      "Median Valid ensemble score:  0.8686874035674178\n"
     ]
    }
   ],
   "source": [
    "list_preds= []\n",
    "preds = model_gcn.predict(test_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.median(preds_arr, axis=0)\n",
    "print(\"Median Test ensemble score: \", roc_auc_score(test_dataset.y, ensemble_preds, average=\"macro\"))\n",
    "\n",
    "\n",
    "\n",
    "list_preds = []\n",
    "preds = model_gcn.predict(valid_dataset)\n",
    "list_preds.append(preds)\n",
    "preds_arr = np.asarray(list_preds)\n",
    "ensemble_preds = np.median(preds_arr, axis=0)\n",
    "print(\"Median Valid ensemble score: \", roc_auc_score(valid_dataset.y, ensemble_preds, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
